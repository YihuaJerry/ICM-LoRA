# @title Imports

import argparse
from ast import arg
import csv
import os
import re
from sympy import true
import torch

import numpy as np
import supervision as sv
from transformers import (
    AutoModelForCausalLM,
    AutoProcessor
)
from roboflow import Roboflow
from peft import PeftModel
from peft import PeftModel, PeftConfig
from data.florence_detection_dataset import DetectionDataset

parser = argparse.ArgumentParser("test cvae generated lora or finetuned lora")
parser.add_argument("--download_location", required=True, help='download dataset location',default= "/home/ma-user/work/ymx/data/voc2012")
parser.add_argument("--datasetname",type=str,required=True,help="datasetname,such as chair,dog,cat")
parser.add_argument("--generated_lora",type=str,required=True,help="lora weight generated by CVAE",default="/home/ma-user/work/ymx/Neural-Network-Parameter-Diffusion-main/florence2-lora/dog-cvae")
parser.add_argument('--rank',type=str,default="8",required=True)
args = parser.parse_args()
download_location = args.download_location
datasetname = args.datasetname
generated_lora = args.generated_lora

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
EXAMPLE_IMAGE_PATH = "dog.jpeg"
CHECKPOINT = "microsoft/Florence-2-base-ft"
REVISION = 'refs/pr/6'
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

BATCH_SIZE = 6
NUM_WORKERS = 0

def collate_fn(batch):
    questions, answers, images = zip(*batch)
    inputs = processor(text=list(questions), images=list(images), return_tensors="pt", padding=True).to(DEVICE)
    return inputs, answers

#TODO: 修改数据集位置 e.g. bicycle, bus
train_dataset = DetectionDataset(
    jsonl_file_path = f"{download_location}/train/annotations_" + datasetname +  ".jsonl",
    image_directory_path = f"{download_location}/train/"
)
val_dataset = DetectionDataset(
    jsonl_file_path = f"{download_location}/valid/annotations_"+ datasetname +  ".jsonl",
    image_directory_path = f"{download_location}/valid/"
)


#TODO: 修改为对应的生成模型的位置
peft_model_id = generated_lora
config = PeftConfig.from_pretrained(peft_model_id)
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path).to(DEVICE)
model = PeftModel.from_pretrained(model, peft_model_id).to(DEVICE)
processor = AutoProcessor.from_pretrained(config.base_model_name_or_path, trust_remote_code=True, revision=REVISION)


PATTERN = r'([a-zA-Z0-9_ ]+)<loc_\d+>'

"""
    EXTRACT CLASSES FORM DATASET
"""
def extract_classes(dataset: DetectionDataset):
    class_set = set()
    for i in range(len(dataset.dataset)):
        image, data = dataset.dataset[i]
        suffix = data["suffix"]
        classes = re.findall(PATTERN, suffix)
        class_set.update(classes)
    return sorted(class_set)

CLASSES = extract_classes(train_dataset)

targets = []
predictions = []

for i in range(len(val_dataset.dataset)):
    image, data = val_dataset.dataset[i]
    prefix = data['prefix']
    suffix = data['suffix']

    inputs = processor(text=prefix, images=image, return_tensors="pt").to(DEVICE)
    generated_ids = model.generate(
        input_ids=inputs["input_ids"],
        pixel_values=inputs["pixel_values"],
        max_new_tokens=1024,
        num_beams=3
    )
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]

    prediction = processor.post_process_generation(generated_text, task='<OD>', image_size=image.size)
    prediction = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, prediction, resolution_wh=image.size)
    prediction = prediction[np.isin(prediction['class_name'], CLASSES)]
    prediction.class_id = np.array([CLASSES.index(class_name) for class_name in prediction['class_name']])
    prediction.confidence = np.ones(len(prediction))

    target = processor.post_process_generation(suffix, task='<OD>', image_size=image.size)
    target = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, target, resolution_wh=image.size)
    target.class_id = np.array([CLASSES.index(class_name) for class_name in target['class_name']])

    targets.append(target)
    predictions.append(prediction)

mean_average_precision = sv.MeanAveragePrecision.from_detections(
    predictions=predictions,
    targets=targets,
)

print(f"map50_95: {mean_average_precision.map50_95:.2f}")
print(f"map50: {mean_average_precision.map50:.2f}")
print(f"map75: {mean_average_precision.map75:.2f}")

# 保存结果到 CSV 文件
result_dir = f"./results/generate_cvae/" + datasetname + "/r="  + args.rank
os.makedirs(result_dir, exist_ok=True)

csv_file_path = os.path.join(result_dir, "results.csv")
header = ['Metric', 'Value']
rows = [
    ['map50_95', f"{mean_average_precision.map50_95:.2f}"],
    ['map50', f"{mean_average_precision.map50:.2f}"],
    ['map75', f"{mean_average_precision.map75:.2f}"]
]

with open(csv_file_path, mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(header)
    writer.writerows(rows)